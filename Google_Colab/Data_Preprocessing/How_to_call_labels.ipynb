{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOOk21zBapk7oxMWI9mLzxX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"iB_5xD2wVLDM"},"outputs":[],"source":["train_data_dir = \"train/images\"\n","train_label_dir = \"train/labels\"\n","valid_data_dir = \"valid/images\"\n","valid_label_dir = \"valid/labels\"\n","\n","train_data_filenames = [os.path.join(train_data_dir, f) for f in os.listdir(train_data_dir) if f.endswith('.jpg')]\n","train_label_filenames = [os.path.join(train_label_dir, f) for f in os.listdir(train_label_dir) if f.endswith('.txt')]\n","\n","valid_data_filenames = [os.path.join(valid_data_dir, f) for f in os.listdir(valid_data_dir) if f.endswith('.jpg')]\n","valid_label_filenames = [os.path.join(valid_label_dir, f) for f in os.listdir(valid_label_dir) if f.endswith('.txt')]\n","\n","train_dataset = tf.data.Dataset.from_tensor_slices((train_data_filenames, train_label_filenames))\n","train_dataset = train_dataset.map(lambda x, y: tuple(tf.py_function(\n","    read_image_and_annotation, [x, y], [tf.float32, tf.int32])), num_parallel_calls=tf.data.AUTOTUNE)\n","train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","valid_dataset = tf.data.Dataset.from_tensor_slices((valid_data_filenames, valid_label_filenames))\n","valid_dataset = valid_dataset.map(lambda x, y: tuple(tf.py_function(\n","    read_image_and_annotation, [x, y], [tf.float32, tf.int32])), num_parallel_calls=tf.data.AUTOTUNE)\n","valid_dataset = valid_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)"]},{"cell_type":"code","source":["from keras.preprocessing.image import ImageDataGenerator\n","from keras.applications.resnet import ResNet101\n","from keras.layers import GlobalAveragePooling2D, Dense\n","from keras.models import Model\n","\n","# 데이터 경로 설정\n","train_dir = 'train/images/'\n","train_label_dir = 'train/labels/'\n","valid_dir = 'valid/images/'\n","valid_label_dir = 'valid/labels/'\n","\n","# 데이터 증강을 위한 ImageDataGenerator 생성\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    horizontal_flip=True,\n","    vertical_flip=True\n",")\n","\n","valid_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# 데이터셋 생성\n","train_generator = train_datagen.flow_from_directory(\n","        train_dir,\n","        target_size=(224, 224),\n","        batch_size=32,\n","        class_mode='categorical')\n","\n","valid_generator = valid_datagen.flow_from_directory(\n","        valid_dir,\n","        target_size=(224, 224),\n","        batch_size=32,\n","        class_mode='categorical')\n","\n","# ResNet101 모델 불러오기\n","base_model = ResNet101(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n","\n","# 모델 구성\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(1024, activation='relu')(x)\n","predictions = Dense(16, activation='softmax')(x)\n","model = Model(inputs=base_model.input, outputs=predictions)\n","\n","# 모델 컴파일 및 학습\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","model.fit(train_generator, epochs=10, validation_data=valid_generator)"],"metadata":{"id":"_H7cbRbKVLqb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","valid_images_path = 'valid/images'\n","valid_labels_path = 'valid/labels'\n","\n","# 이미지 파일 경로와 라벨을 담은 데이터프레임 생성\n","valid_image_files = os.listdir(valid_images_path)\n","valid_labels_files = os.listdir(valid_labels_path)\n","\n","valid_image_files = [os.path.join(valid_images_path, file) for file in valid_image_files]\n","valid_labels_files = [os.path.join(valid_labels_path, file) for file in valid_labels_files]\n","\n","valid_df = pd.DataFrame({'filename': valid_image_files, 'label': valid_labels_files})\n","\n","valid_datagen = ImageDataGenerator(rescale=1./255)\n","\n","valid_generator = valid_datagen.flow_from_dataframe(\n","    dataframe=valid_df,\n","    x_col='filename',\n","    y_col='label',\n","    target_size=(224, 224),\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    shuffle=True\n",")"],"metadata":{"id":"QW2Rb476YRsk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","from tensorflow import keras\n","from sklearn.model_selection import train_test_split\n","\n","train_dir = 'data/train/images/'\n","valid_dir = 'data/valid/images/'\n","train_label_dir = 'data/train/labels/'\n","valid_label_dir = 'data/valid/labels/'\n","\n","# 이미지와 라벨을 저장할 리스트\n","x_train = []\n","y_train = []\n","x_valid = []\n","y_valid = []\n","\n","# 이미지 크기 및 채널\n","img_size = (224, 224)\n","img_channel = 3\n","\n","# train 데이터 로드\n","for filename in sorted(os.listdir(train_dir)):\n","    img_path = os.path.join(train_dir, filename)\n","    label_path = os.path.join(train_label_dir, filename[:-4] + '.txt')\n","    \n","    # 이미지와 라벨링 파일 불러오기\n","    img = cv2.imread(img_path)\n","    img = cv2.resize(img, img_size)  # 이미지 크기 조정\n","    with open(label_path) as f:\n","      label = int(f.readline().strip())\n","    \n","    # 이미지와 라벨을 리스트에 추가\n","    x_train.append(img)\n","    y_train.append(label)\n","\n","# valid 데이터 로드\n","for filename in sorted(os.listdir(valid_dir)):\n","    img_path = os.path.join(valid_dir, filename)\n","    label_path = os.path.join(valid_label_dir, filename[:-4] + '.txt')\n","    \n","    # 이미지와 라벨링 파일 불러오기\n","    img = cv2.imread(img_path)\n","    img = cv2.resize(img, img_size)  # 이미지 크기 조정\n","    with open(label_path) as f:\n","      label = int(f.readline().strip())\n","    \n","    # 이미지와 라벨을 리스트에 추가\n","    x_valid.append(img)\n","    y_valid.append(label)\n","\n","# NumPy 배열로 변환\n","x_train = np.array(x_train)\n","y_train = np.array(y_train)\n","x_valid = np.array(x_valid)\n","y_valid = np.array(y_valid)\n","\n","# 데이터 전처리\n","x_train = np.array(x_train) / 255.\n","x_valid = np.array(x_valid) / 255.\n","y_train = keras.utils.to_categorical(y_train, num_classes=16)\n","y_valid = keras.utils.to_categorical(y_valid, num_classes=16)\n","\n","# 모델 빌드\n","model = keras.applications.ResNet101(include_top=True, weights=None, input_shape=(224, 224, 3), classes=16)\n","\n","# 모델 학습\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_valid, y_valid))"],"metadata":{"id":"RDm0EnxPb6U-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from tensorflow import keras\n","from sklearn.model_selection import train_test_split\n","\n","train_dir = 'data/train/images/'\n","valid_dir = 'data/valid/images/'\n","train_label_dir = 'data/train/labels/'\n","valid_label_dir = 'data/valid/labels/'\n","\n","# train 데이터셋 불러오기\n","train_filenames = os.listdir(train_dir)\n","train_df = pd.DataFrame({'filename': train_filenames})\n","train_df['label'] = [int(open(train_label_dir+filename[:-4]+'.txt').readline().strip().split()[0]) for filename in train_filenames]\n","\n","# validation 데이터셋 불러오기\n","valid_filenames = os.listdir(valid_dir)\n","valid_df = pd.DataFrame({'filename': valid_filenames})\n","valid_df['label'] = [int(open(valid_label_dir+filename[:-4]+'.txt').readline().strip().split()[0]) for filename in valid_filenames]\n","\n","# 데이터 전처리\n","img_size = (224, 224)\n","batch_size = 32\n","\n","train_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n","train_generator = train_datagen.flow_from_dataframe(\n","    train_df,\n","    directory=train_dir,\n","    x_col='filename',\n","    y_col='label',\n","    target_size=img_size,\n","    batch_size=batch_size,\n","    class_mode='categorical'\n",")\n","\n","valid_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n","valid_generator = valid_datagen.flow_from_dataframe(\n","    valid_df,\n","    directory=valid_dir,\n","    x_col='filename',\n","    y_col='label',\n","    target_size=img_size,\n","    batch_size=batch_size,\n","    class_mode='categorical'\n",")\n","\n","# 모델 빌드\n","model = keras.applications.ResNet101(include_top=True, weights=None, input_shape=(224, 224, 3), classes=16)\n","\n","# 모델 학습\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.fit(train_generator, epochs=10, batch_size=batch_size, validation_data=valid_generator)"],"metadata":{"id":"uhLBwMn-eUTa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["전처리를 하는 방법에 따라 차이가 있습니다. 코드에서 보여지는 전처리 방법은 데이터를 모두 메모리에 불러와서 NumPy 배열로 변환한 후, 각각의 이미지와 레이블에 대해서 전처리를 진행한 뒤 다시 NumPy 배열로 변환하는 방법입니다. 이 방법은 데이터셋이 작을 때나, 이미지 개수가 적을 때에는 사용할 수 있지만, 큰 데이터셋의 경우에는 메모리를 많이 차지하여 out-of-memory (OOM) 에러가 발생할 수 있습니다.\n","\n","반면, flow_from_dataframe() 함수는 데이터를 배치(batch) 단위로 처리하기 때문에 전체 데이터셋을 메모리에 불러오지 않아도 됩니다. 이 함수는 주어진 데이터프레임으로부터 이미지 경로와 레이블을 읽어와서 배치(batch) 단위로 이미지를 불러와 전처리를 수행합니다. 이 방법은 데이터셋의 크기가 클 때 효율적으로 처리할 수 있습니다."],"metadata":{"id":"HfmrNjGnsBki"}}]}